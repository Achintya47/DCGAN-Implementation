{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!kaggle datasets download -d diraizel/anime-images-dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T11:42:35.518825Z","iopub.execute_input":"2025-07-11T11:42:35.519336Z","iopub.status.idle":"2025-07-11T11:42:44.776104Z","shell.execute_reply.started":"2025-07-11T11:42:35.519304Z","shell.execute_reply":"2025-07-11T11:42:44.775345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\nwith zipfile.ZipFile(\"/kaggle/working/anime-images-dataset.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"anime-images-dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T11:43:13.948734Z","iopub.execute_input":"2025-07-11T11:43:13.949071Z","iopub.status.idle":"2025-07-11T11:43:27.168432Z","shell.execute_reply.started":"2025-07-11T11:43:13.949041Z","shell.execute_reply":"2025-07-11T11:43:27.167816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom tqdm import tqdm  # Make sure to install tqdm if you haven't already\n\nsource_dir = r'/kaggle/working/anime-images-dataset/data/anime_images'\ndestination_dir = r'/kaggle/working/anime-images/images'\n\n# Create destination folder if it doesn't exist\nos.makedirs(destination_dir, exist_ok=True)\n\n# Count total files for tqdm\nall_files = []\nfor root, dirs, files in os.walk(source_dir):\n    for file in files:\n        all_files.append(os.path.join(root, file))\n\nfor file_path in tqdm(all_files, desc=\"Moving images\"):\n    file = os.path.basename(file_path)\n    dest_path = os.path.join(destination_dir, file)\n    \n    # Handle file name conflicts by appending a counter\n    counter = 1\n    while os.path.exists(dest_path):\n        base, ext = os.path.splitext(file)\n        dest_path = os.path.join(destination_dir, f\"{base}_{counter}{ext}\")\n        counter += 1\n\n    shutil.copy2(file_path, dest_path)\n\nprint(\"âœ… All images have been moved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T11:44:46.653299Z","iopub.execute_input":"2025-07-11T11:44:46.654051Z","iopub.status.idle":"2025-07-11T11:44:56.493378Z","shell.execute_reply.started":"2025-07-11T11:44:46.654025Z","shell.execute_reply":"2025-07-11T11:44:56.492617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport numpy as np\nimport torch.nn.parallel\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T11:01:40.464123Z","iopub.execute_input":"2025-07-11T11:01:40.464389Z","iopub.status.idle":"2025-07-11T11:01:48.450223Z","shell.execute_reply.started":"2025-07-11T11:01:40.464364Z","shell.execute_reply":"2025-07-11T11:01:48.449413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_root=\"/kaggle/working/anime-images\"\n\nworkers = 2\n\nbatch_size = 128\n\nimage_size = 64\n\nn_channel = 3\n\nn_z_noise = 100\n\nn_feature_generator = 64\n\nepochs = 10\n\nlr = 0.002\n\nbeta1 = 0.5\n\nngpu = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T11:45:14.662793Z","iopub.execute_input":"2025-07-11T11:45:14.663091Z","iopub.status.idle":"2025-07-11T11:45:14.667301Z","shell.execute_reply.started":"2025-07-11T11:45:14.663072Z","shell.execute_reply":"2025-07-11T11:45:14.666425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset=datasets.ImageFolder(root=data_root,\n                            transform=transforms.Compose([\n                                transforms.Resize(image_size),\n                                transforms.CenterCrop(image_size),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5,0.5,0.5) , (0.5,0.5,0.5)),\n                            ]))\n\ndataloader=torch.utils.data.DataLoader(dataset , batch_size=batch_size,\n                                       shuffle=True,num_workers=workers)\n\ndevice=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nreal_batch=next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T11:45:18.773974Z","iopub.execute_input":"2025-07-11T11:45:18.774270Z","iopub.status.idle":"2025-07-11T11:45:19.780179Z","shell.execute_reply.started":"2025-07-11T11:45:18.774251Z","shell.execute_reply":"2025-07-11T11:45:19.779304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import Type\ndef init_weights(model : Type[nn.Module]) -> None :\n    '''\n    Function to Custom-Initialize the weights of the model\n    according to the official research paper : arXiv:1511.06434v2\n\n    Parameters : \n        mode : subclass of the torch.nn.Module , the model\n    '''\n    for m in model.modules():\n        # If any of these layers occurs , the initialize the weights with 0.02 as std dev and 0.0 as mean\n        if isinstance(m , (nn.Conv2d , nn.ConvTranspose2d , nn.BatchNorm2d)):\n            nn.init.normal_(m.weight.data , 0.0 , 0.02)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T11:45:29.520330Z","iopub.execute_input":"2025-07-11T11:45:29.521333Z","iopub.status.idle":"2025-07-11T11:45:29.526323Z","shell.execute_reply.started":"2025-07-11T11:45:29.521293Z","shell.execute_reply":"2025-07-11T11:45:29.525475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self,ngpu):\n        '''\n        Initialize the Generator class\n        '''\n        super(Generator , self).__init__()\n        self.ngpu=ngpu\n        # Generator architecture\n        self.main=nn.Sequential(\n            # input is Z , going into convolutional transpose\n            nn.ConvTranspose2d(n_z_noise , n_feature_generator * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(n_feature_generator * 8),\n            nn.ReLU(True),\n\n            # state size : (ngf*8) x 4 x 4 -> (64*8) x 4 x 4 \n            nn.ConvTranspose2d(n_feature_generator * 8, n_feature_generator * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(n_feature_generator * 4),\n            nn.ReLU(True),\n\n            # state size : (ngf*4) x 8 x 8 -> (64*4) x 8 x 8 \n            nn.ConvTranspose2d( n_feature_generator * 4, n_feature_generator * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(n_feature_generator * 2),\n            nn.ReLU(True),\n\n            # state size : (ngf*2) x 16 x 16 -> (64*2) x 16 x 16 \n            nn.ConvTranspose2d( n_feature_generator * 2, n_feature_generator, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(n_feature_generator),\n            nn.ReLU(True), \n            \n            # state size : (ngf) x 32 x 32 -> (64) x 32 x 32 \n            nn.ConvTranspose2d( n_feature_generator, n_channel, 4, 2, 1, bias=False),\n            nn.Tanh()\n\n            # state size : n_channel x 64 x 64 -> 3 x 64 x 64\n        )\n\n    def forward(self, input : torch.Tensor) -> torch.Tensor :\n        '''\n        Forward pass for the generator\n\n        Parameters : \n            input : A noise tensor of shape n_z_noise\n        \n        Returns : \n            torch.Tensor : a generated image of shape 3 x 64 x 64\n        '''\n        return self.main(input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T11:45:31.026865Z","iopub.execute_input":"2025-07-11T11:45:31.027686Z","iopub.status.idle":"2025-07-11T11:45:31.033856Z","shell.execute_reply.started":"2025-07-11T11:45:31.027652Z","shell.execute_reply":"2025-07-11T11:45:31.033108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gen_net=Generator(ngpu).to(device)\n\nif (device.type=='cuda') and (ngpu > 1):\n    gen_net=nn.DataParallel(gen_net , list(range(ngpu)))\n\ngen_net.apply(init_weights)\n\nprint(gen_net)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T11:45:33.544692Z","iopub.execute_input":"2025-07-11T11:45:33.545438Z","iopub.status.idle":"2025-07-11T11:45:33.580758Z","shell.execute_reply.started":"2025-07-11T11:45:33.545413Z","shell.execute_reply":"2025-07-11T11:45:33.580026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Discriminator(nn.Module): \n    def __init__(self , ngpu):\n        super(Discriminator , self).__init__()\n        self.ngpu=ngpu\n\n        self.main=nn.Sequential(\n\n            nn.Conv2d(n_channel , n_feature_generator , 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2 , inplace=True),\n\n            nn.Conv2d(n_feature_generator, n_feature_generator * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(n_feature_generator * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(n_feature_generator * 2, n_feature_generator * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(n_feature_generator * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(n_feature_generator * 4, n_feature_generator * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(n_feature_generator * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(n_feature_generator * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()           \n        )\n\n    def forward(self , input):\n        return self.main(input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T11:45:35.396733Z","iopub.execute_input":"2025-07-11T11:45:35.397584Z","iopub.status.idle":"2025-07-11T11:45:35.403595Z","shell.execute_reply.started":"2025-07-11T11:45:35.397555Z","shell.execute_reply":"2025-07-11T11:45:35.402986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dis_net=Discriminator(ngpu).to(device)\n\nif (device.type == 'cuda') and (ngpu > 1):\n    dis_net = nn.DataParallel(dis_net, list(range(ngpu)))\n\ndis_net.apply(init_weights)\n\nprint(dis_net)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T11:45:40.340454Z","iopub.execute_input":"2025-07-11T11:45:40.340718Z","iopub.status.idle":"2025-07-11T11:45:40.369293Z","shell.execute_reply.started":"2025-07-11T11:45:40.340699Z","shell.execute_reply":"2025-07-11T11:45:40.368473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion=nn.BCELoss()\n\nfixed_input_noise = torch.randn(64, n_z_noise, 1, 1, device=device)\n\nreal_label = 1\nfake_label = 0\n\noptimizer_dis = optim.Adam(dis_net.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizer_gen = optim.Adam(gen_net.parameters(), lr=lr, betas=(beta1, 0.999))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T11:45:42.366820Z","iopub.execute_input":"2025-07-11T11:45:42.367096Z","iopub.status.idle":"2025-07-11T11:45:42.372676Z","shell.execute_reply.started":"2025-07-11T11:45:42.367075Z","shell.execute_reply":"2025-07-11T11:45:42.372146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_list=[]\ngen_losses=[]\ndis_losses=[]\niters=0\n\nwriter_real=SummaryWriter(f\"logs/real\")\nwriter_fake=SummaryWriter(f\"logs/fake\")\n\n\nprint(\"Starting Training Loop....\")\n\nfor epoch in range(epochs):\n\n    for i , data in enumerate(dataloader , 0):\n\n        dis_net.zero_grad()\n\n        real_to_cpu = data[0].to(device)\n        b_size=real_to_cpu.size(0)\n        label=torch.full((b_size,), real_label, dtype=torch.float, device=device)\n\n        output=dis_net(real_to_cpu).view(-1)\n\n        errorD_real = criterion(output, label)\n\n        errorD_real.backward()\n        D_x=output.mean().item()\n\n        noise = torch.randn(b_size, n_z_noise, 1, 1, device=device)\n\n        fake=gen_net(noise)\n        label.fill_(fake_label)\n\n        output = dis_net(fake.detach()).view(-1)\n\n        errorD_fake = criterion(output, label)\n\n        errorD_fake.backward()\n        optimizer_dis.step()\n\n        D_G_z1= output.mean().item()\n\n        errorD=errorD_fake + errorD_real\n\n        optimizer_dis.step()\n\n        gen_net.zero_grad()\n        label.fill_(real_label)\n\n        output = dis_net(fake).view(-1)\n        errorG = criterion(output, label)\n\n        errorG.backward()\n        D_G_z2 = output.mean().item()\n\n        optimizer_gen.step()\n\n        if i%50==0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, epochs, i, len(dataloader),\n                     errorD.item(), errorG.item(), D_x, D_G_z1, D_G_z2))\n            \n            gen_losses.append(errorG.item())\n            dis_losses.append(errorD.item())\n            \n            if (iters % 500 == 0) or ((epoch == epochs-1) and (i == len(dataloader)-1)):\n                with torch.no_grad():\n                    fake = gen_net(fixed_input_noise).detach().cpu()\n                img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n            img_grid_real = vutils.make_grid(real_to_cpu[:32],normalize=True)\n            img_grid_fake = vutils.make_grid(fake[:32],normalize=True)\n\n            writer_real.add_image(\"Real\", img_grid_real, global_step=iters)\n            writer_fake.add_image(\"Fake\", img_grid_fake, global_step=iters)\n\n            iters += 1\n        \n\n# Save everything needed for resuming\ntorch.save({\n    'epoch': epoch,\n    'gen_state_dict': gen_net.state_dict(),\n    'dis_state_dict': dis_net.state_dict(),\n    'optimizer_gen': optimizer_gen.state_dict(),\n    'optimizer_dis': optimizer_dis.state_dict(),\n    'gen_losses': gen_losses,\n    'dis_losses': dis_losses,\n    'iters': iters\n}, 'gan_checkpoint.pth')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T11:45:44.070764Z","iopub.execute_input":"2025-07-11T11:45:44.071066Z","iopub.status.idle":"2025-07-11T11:56:13.777348Z","shell.execute_reply.started":"2025-07-11T11:45:44.071047Z","shell.execute_reply":"2025-07-11T11:56:13.776573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(gen_losses,label=\"G\")\nplt.plot(dis_losses,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T12:06:27.990640Z","iopub.execute_input":"2025-07-11T12:06:27.991423Z","iopub.status.idle":"2025-07-11T12:06:28.168633Z","shell.execute_reply.started":"2025-07-11T12:06:27.991389Z","shell.execute_reply":"2025-07-11T12:06:28.167895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.animation as animation\nfrom IPython.display import HTML\n\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T11:29:26.762276Z","iopub.execute_input":"2025-07-11T11:29:26.763012Z","iopub.status.idle":"2025-07-11T11:29:27.371461Z","shell.execute_reply.started":"2025-07-11T11:29:26.762984Z","shell.execute_reply":"2025-07-11T11:29:27.370569Z"}},"outputs":[],"execution_count":null}]}